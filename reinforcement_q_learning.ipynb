{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Batch Mode Reinforcement Learning DQN PytTorch - Mountaincar\n",
    "=====================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#env = gym.make('MountainCar-v0').unwrapped\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "\n",
    "or this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN algorithm\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 150)\n",
    "        self.fc2 = nn.Linear(150, 75)\n",
    "        self.fc3 = nn.Linear(75, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Return Q values\n",
    "        return x # num_actions, gives the Q value for each\n",
    "        \n",
    "        # return action probabilities\n",
    "        #action_probs = F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "Hyperparameters and utilities\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-  ``Variable`` - this is a simple wrapper around\n",
    "   ``torch.autograd.Variable`` that will automatically send the data to\n",
    "   the GPU every time we construct a Variable.\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the durations of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN(2, 3) # state_dim, num_actions\n",
    "target_net = DQN(2, 3) # state_dim, num_actions\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    policy_net = policy_net.cuda()\n",
    "    target_net = target_net.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.01)\n",
    "memory = ReplayMemory(200000) # 13361 willl be used according to my tets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Not being used here. These are for a stochastic setup\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action_stochastic(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        action_probs = policy_net(\n",
    "            Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(3)]])\n",
    "    \n",
    "def select_action(state):\n",
    "    action_probs = policy_net(\n",
    "            Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "^^^^^^^^^^^^^\n",
    "\n",
    "\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state. We also use a target network to compute $V(s_{t+1})$ for\n",
    "added stability. The target network has its weights kept frozen most of\n",
    "the time, but is updated with the policy network's weights every so often.\n",
    "This is usually a set number of steps but we shall use episodes for\n",
    "simplicity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    #Terminal position is 0.5\n",
    "    return state[0] >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample transitions from buffer\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: not is_terminal(s),\n",
    "                                          batch.next_state)))\n",
    "\n",
    "    # Select all s' that are not final\n",
    "    non_final_next_states = Variable(torch.cat([FloatTensor(s) for s in batch.next_state\n",
    "                                                if not is_terminal(s)]),\n",
    "                                     volatile=True)\n",
    "       \n",
    "    state_batch = Variable(torch.cat([FloatTensor(s) for s in batch.state]))\n",
    "    action_batch = Variable(LongTensor(batch.action))\n",
    "    reward_batch = Variable(FloatTensor(batch.reward))\n",
    "    \n",
    "    batch_size = min(BATCH_SIZE,state_batch.size()[0]/2) #Position vel pair hence divide by 2.\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    \n",
    "    # Returns [Q(s,a_1), Q(s,a_2), Q(s,a_3)] for each state\n",
    "    Q_values_all_actions = policy_net(state_batch.view(batch_size,2))\n",
    "    \n",
    "    # Returns [Q(s, a_i)] where a_i comes from action_batch\n",
    "    Q_values_selected_actions = Q_values_all_actions.gather(1, action_batch.view(-1, 1))\n",
    "    \n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(batch_size).type(Tensor))\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    #.max(1) does an argmax\n",
    "    \n",
    "    next_state_values[non_final_mask] = (target_net(non_final_next_states.view(-1,2)).max(1)[0])\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_Q_values_selected_actions = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Undo volatility (which was used to prevent unnecessary gradients)\n",
    "    expected_Q_values_selected_actions = Variable(expected_Q_values_selected_actions.data)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.mse_loss(Q_values_selected_actions, expected_Q_values_selected_actions)\n",
    "\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect off-policy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstd import LSTDQ, LSTDMu, LSPI\n",
    "from simulator import Simulator\n",
    "from policy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# pi_expert\n",
    "\n",
    "\n",
    "# pi_explore\n",
    "pi1 = RandomPolicy2(choices=[0]) # left\n",
    "pi2 = RandomPolicy2(choices=[2]) # right\n",
    "pi3 = RandomPolicy2(choices=[0, 2]) # left, right\n",
    "\n",
    "class ManualPolicy():\n",
    "    def choose_action(self, s):\n",
    "        pos, v = s\n",
    "        return 0 if v <=0 else 2\n",
    "    \n",
    "class StochasticPolicy():\n",
    "    def __init__(self):\n",
    "        self.manual_policy = ManualPolicy()\n",
    "        self.random_policy = RandomPolicy2(choices=[0, 2]) # left, right\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        if random.random() < 0.25:\n",
    "            return self.random_policy.choose_action(s)\n",
    "        else:\n",
    "            return self.manual_policy.choose_action(s)\n",
    "    \n",
    "pi4 = ManualPolicy()\n",
    "\n",
    "# pi_evaluate\n",
    "pi5 = RandomPolicy2(choices=[0, 1, 2]) # left, right\n",
    "\n",
    "# for offline dqn\n",
    "pi6 = StochasticPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "# discrete action\n",
    "action_dim = 1\n",
    "n_action = env.action_space.n\n",
    "sim = Simulator(env, state_dim=state_dim, action_dim=action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/sbssbh/josh/mighty-rl/simulator.py(47)simulate()\n",
      "-> env = self._env\n",
      "(Pdb) continue\n"
     ]
    }
   ],
   "source": [
    "trajs_for_dqn = sim.simulate(pi4, n_trial=1, n_episode=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of trajs_for_dqn = 1000 x ~88\n"
     ]
    }
   ],
   "source": [
    "print(\"dimensions of trajs_for_dqn = {0} x ~{1}\".format(len(trajs_for_dqn), len(trajs_for_dqn[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of transitions in trajs_for_dqn = 129363\n"
     ]
    }
   ],
   "source": [
    "num_transitions = 0\n",
    "for episode in trajs_for_dqn:\n",
    "    num_transitions += len(episode)\n",
    "print(\"number of transitions in trajs_for_dqn = {0}\".format(num_transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example transition: Transition(s=array([-0.42763408,  0.        ]), a=0, r=-1.0, s_next=array([-0.42934391, -0.00170983]), done=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"example transition: {0}\".format(trajs_for_dqn[0][0]))\n",
    "terminal_position = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In our approach, we store *all* transition in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in trajs_for_dqn:\n",
    "    for transition in episode:        \n",
    "        # Store the transition in memory        \n",
    "        memory.push(transition.s, transition.a, transition.s_next, transition.r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [06:06<00:59,  4.26s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "loss = []\n",
    "NUM_ITERATIONS = 100\n",
    "for iteration in tqdm(range(NUM_ITERATIONS)):\n",
    "    loss.append(optimize_model().data[0])\n",
    "    if iteration % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "plt.plot(range(len(loss)), loss)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper for the agent\n",
    "# Wrapper so that choose_action is exposed\n",
    "class Dqn_wrapper(object):\n",
    "    def __init__(self, policy_net):\n",
    "        self.policy_net = policy_net\n",
    "        self.policy_net.eval()\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        action = self.policy_net(Variable(FloatTensor(s))).max(0)[1]\n",
    "        return action.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_dqn = Dqn_wrapper(policy_net)\n",
    "D = sim.simulate(pi_dqn, 1, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', palette='Blues')\n",
    "def plot_trajectory_mountain_car(D, noshow=False):\n",
    "    \"\"\"TODO: Docstring for plot_trajectories.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : TODO\n",
    "    Returns\n",
    "    -------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.xticks(fontsize=15.0)\n",
    "    plt.yticks(fontsize=15.0)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    lines = [[(-0.4, 0.0), (-0.6, 0.0)]]\n",
    "    c = np.array([(1, 0, 0, 1)])\n",
    "    lc = mc.LineCollection(lines, colors='red', linewidths=5)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    for episode in D:\n",
    "        states = []\n",
    "        for (s, a, r, s_next, done) in episode:\n",
    "            states.append(s)\n",
    "        states = np.array(states)\n",
    "        sc = ax.scatter(states[:,0], states[:,1], c=range(len(states[:,0])), cmap=plt.get_cmap(\"YlOrRd\"), s=7.0)\n",
    "    ax.set_xlim(-1.3, 0.6)\n",
    "    ax.axvline(0.5, c='blue', linewidth=5)\n",
    "    ax.set_xlabel('Position', fontsize=20.0)\n",
    "    ax.set_ylabel('Velocity', fontsize=20.0)\n",
    "\n",
    "    if noshow:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory_mountain_car(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
